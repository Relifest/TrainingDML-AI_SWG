== Introduction

PyTDML is a pure Python parser and encoder for training datasets based on the OGC Training Data Markup Language for AI standard. Welcome to the PyTDML tutorial, your guide to working with Training Data Markup Language (TDML) for Artificial Intelligence.

=== What is TDML?

The Training Data Markup Language for Artificial Intelligence (TrainingDML-AI) SWG is chartered to develop the UML model and encodings for geospatial machine learning training data. In machine learning, training data is the dataset used for the training and validation of machine learning models. The geospatial training data categories will include, but are not limited to, remote sensing imagery, moving features (e.g., vehicle trajectories), and related spatial content. The SWG will define a UML model and encodings consistent with the OGC standards baseline to exchange and retrieve the training data in the web environment. The SWG will start with the JSON implementation. Once the UML and JSON encoding are well accepted, the SWG will work on the XML encoding as per the current OGC baseline. This standard will provide more detailed metadata for formalizing the information model of training data.

=== Main Functions of PyTDML

PyTDML is a framework or library used for processing and managing training datasets. The following figure shows the structure of classes implemented by PyTDML.

image::Implementation.png[]

The main functions of PyTDML can be summarized as follows:

- Dataset Definition and Management: PyTDML allows users to organize and describe training datasets by defining classes such as  _TrainingDataset_ and _Task_. Users can manually input metadata information, which can include the ID, name, description, task type, data list, version, creation and update time, provider, keywords, data source, and specific category of the dataset. This enables the integration of training data from different sources into a  _TrainingDataset_ object.
- Task Definition: In PyTDML, users can define different tasks including "Scene Classification" and "Semantic Segmentation" and provide detailed descriptions for these tasks. This helps clarify the purpose and scenario of using the dataset, facilitating subsequent data processing and analysis.
- Data Pipeline: PyTDML has designed specific data pipeline routes for different Earth observation missions. Each data pipeline is responsible for loading the dataset required for the corresponding task. Within the same task type, data pipelines can use standardized loading logic to parse and process different datasets, thanks to the shared composition patterns of training samples among these datasets.
- Data Annotation and Classification: PyTDML supports the annotation and classification of training data. For example, in remote sensing image datasets, specific categories such as "Airport," "Beach," "Bridge," etc., can be defined, and data can be organized through these categories.
- Data Standardization: By defining a unified format for the dataset (such as image size, band information, etc.), PyTDML helps to standardize data processing, thereby improving data consistency and comparability.
- Metadata Management: PyTDML emphasizes the importance of metadata and provides convenience for the long-term storage and reuse of datasets by detailing the metadata of the dataset, such as data source, creator, update time, etc.

Letâ€™s dive in and start exploring the world of standardized geospatial training data with PyTDML!

